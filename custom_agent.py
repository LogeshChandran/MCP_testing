# Assuming you have langchain and other dependencies installed
from typing import Any, Dict, Optional, Union, Tuple, Callable, List
from pydantic import Field, PrivateAttr, BaseModel, ValidationError
from typing_extensions import Literal, InstanceOf
from langchain.llms.base import BaseLLM
from langchain.tools import BaseTool
from langchain.prompts import PromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.agents import AgentExecutor
import datetime
from langchain.memory import ConversationBufferMemory
import logging
import json
# from jsonschema import validate # You might need this if not using pydantic for validation


class StructuredOutput(BaseModel):
    """Base class for structured outputs."""
    output: str


class SummaryOutput(StructuredOutput):
    """Output for summarization tasks."""
    summary: str = Field(..., description="A concise summary of the input text.")


class FactExtractionOutput(StructuredOutput):
    """Output for fact extraction tasks."""
    facts: List[str] = Field(..., description="A list of extracted facts.")


class Agent:  # Removed BaseAgent inheritance
    """Represents an agent in a system with memory, dynamic tool selection, enhanced prompts, multi-agent collaboration, logging, and guardrails."""

    _times_executed: int = PrivateAttr(default=0)
    max_execution_time: Optional[int] = Field(
        default=None,
        description="Maximum execution time for an agent to execute a task",
    )
    agent_ops_agent_name: str = None  # type: ignore
    agent_ops_agent_id: str = None  # type: ignore
    step_callback: Optional[Any] = Field(
        default=None,
        description="Callback to be executed after each step of the agent execution.",
    )
    use_system_prompt: Optional[bool] = Field(
        default=True,
        description="Use system prompt for the agent.",
    )
    llm: Union[str, InstanceOf[BaseLLM], Any] = Field(
        description="Language model that will run the agent.", default=None
    )
    function_calling_llm: Optional[Union[str, InstanceOf[BaseLLM], Any]] = Field(
        description="Language model that will run the agent.", default=None
    )
    system_template: Optional[str] = Field(
        default=None, description="System format for the agent."
    )
    prompt_template: Optional[str] = Field(
        default=None, description="Prompt format for the agent."
    )
    response_template: Optional[str] = Field(
        default=None, description="Response format for the agent."
    )
    allow_code_execution: Optional[bool] = Field(
        default=False, description="Enable code execution for the agent."
    )
    respect_context_window: bool = Field(
        default=True,
        description="Keep messages under the context window size by summarizing content.",
    )
    max_retry_limit: int = Field(
        default=2,
        description="Maximum number of retries for an agent to execute a task when an error occurs.",
    )
    multimodal: bool = Field(
        default=False,
        description="Whether the agent is multimodal.",
    )
    inject_date: bool = Field(
        default=False,
        description="Whether to automatically inject the current date into tasks.",
    )
    date_format: str = Field(
        default="%Y-%m-%d",
        description="Format string for date when inject_date is enabled.",
    )
    code_execution_mode: Literal["safe", "unsafe"] = Field(
        default="safe",
        description="Mode for code execution: 'safe' (using Docker) or 'unsafe' (direct execution).",
    )
    reasoning: bool = Field(
        default=False,
        description="Whether the agent should reflect and create a plan before executing a task.",
    )
    max_reasoning_attempts: Optional[int] = Field(
        default=None,
        description="Maximum number of reasoning attempts before executing the task. If None, will try until ready.",
    )
    embedder: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Embedder configuration for the agent.",
    )
    agent_knowledge_context: Optional[str] = Field(
        default=None,
        description="Knowledge context for the agent.",
    )
    crew_knowledge_context: Optional[str] = Field(
        default=None,
        description="Knowledge context for the crew.",
    )
    knowledge_search_query: Optional[str] = Field(
        default=None,
        description="Knowledge search query for the agent dynamically generated by the agent.",
    )
    from_repository: Optional[str] = Field(
        default=None,
        description="The Agent's role to be used from your repository.",
    )
    guardrail: Optional[Union[Callable[[Any, Tuple[bool, Any]], str]] = Field(
        default=None,
        description="Function or string description of a guardrail to validate agent output"
    )
    guardrail_max_retries: int = Field(
        default=3, description="Maximum number of retries when guardrail fails"
    )

    role: str = Field(description="The role of the agent.")
    goal: str = Field(description="The objective of the agent.")
    backstory: str = Field(description="The backstory of the agent.")
    knowledge: Optional[str] = Field(default=None, description="The knowledge base of the agent.")
    tools: List[BaseTool] = Field(default_factory=list, description="Tools at agents disposal")
    verbose: bool = Field(default=False, description="Whether the agent execution should be in verbose mode.")
    system_template: str = Field(default="""You are a helpful agent. \n {agent_scratchpad}""", description="System format for the agent.")
    prompt_template: str = Field(default="""Use the following tools if required to answer the question below:\n{tools}\n\nQuestion: {input}""", description="Prompt format for the agent.")
    memory: Optional[ConversationBufferMemory] = Field(default_factory=ConversationBufferMemory, description="Memory for the agent")
    allow_delegation: bool = Field(default=False, description="Whether the agent is allowed to delegate tasks to other agents.")
    sub_agents: List[Any] = Field(default_factory=list, description="List of sub-agents to delegate tasks to.")  # Type should be Agent but causes circular import
    logger: logging.Logger = Field(default_factory=logging.getLogger, description="Logger for the agent")
    output_format: str = Field(default="SummaryOutput", description="The expected output format of the agent.")

    def __post_init__(self):
        """Initializes the logger and sets the default guardrail."""
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

        if self.guardrail is None:
            self.guardrail = self.default_guardrail  # Set the default guardrail

    def create_langchain_agent(self):
        """Creates a Langchain agent with memory, dynamic tool selection, and enhanced prompts."""

        if isinstance(self.llm, str):
            raise ValueError("LLM must be a Langchain LLM instance, not a string.")

        current_date = datetime.datetime.now()
        formatted_date = current_date.strftime(self.date_format)
        day_of_week = current_date.strftime("%A")

        role_prompt = self.role if self.role else "a helpful assistant"
        goal_prompt = self.goal if self.goal else "to assist the user to the best of your ability"

        system_prompt_content = (
            f"You are {role_prompt}. Your goal is {goal_prompt}. Your backstory is {self.backstory}. "
            f"Today is {day_of_week}, {formatted_date}."
        )
        if self.knowledge:
            system_prompt_content += f" Your knowledge base: {self.knowledge}"

        system_prompt = PromptTemplate.from_template(template=system_prompt_content + "\n{agent_scratchpad}", template_format="jinja2")

        prompt = PromptTemplate.from_template(template=self.prompt_template + "\n Output in JSON format following this schema: {output_format}", partials={"tools":  "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools]), "output_format": self.output_format}, template_format="jinja2")

        # Dynamic Tool Selection (Simplified - always uses all tools)
        def get_tools(tools: List[BaseTool], query: str):
            return tools  # In a real implementation, you would use the query to select tools

        # Memory integration
        memory = self.memory

        def load_memory(input):
            return memory.load_memory_variables(input)

        def save_memory(output):
            memory.save_context({"input": output['input']}, {"output": output['output']})
            return output

        # Chain definition
        chain = RunnablePassthrough.assign(tools=get_tools, input=lambda x: x["input"]) | RunnablePassthrough.assign(**load_memory({"input": lambda x: x['input']})) | prompt | system_prompt | self.llm | StrOutputParser()

        def save_output(output):
          save_memory({"input": output['input'], "output": output['output']})
          return output

        return chain

    def create_agent_executor(self):
      chain = self.create_langchain_agent()
      tools = {tool.name: tool for tool in self.tools}

      def run_tool(tool_name: str, tool_input: str):
        return tools[tool_name.run(tool_input)

      agent_chain = chain.assign(tool_names=list(tools.keys()), run_tool=run_tool)
      return AgentExecutor(agent=agent_chain, tools=self.tools, verbose=self.verbose, memory=self.memory)

    def delegate_task(self, task: str, sub_agent: Any) -> str:  # Type should be Agent but causes circular import
        """Delegates a task to a sub-agent."""
        if not self.allow_delegation:
            raise ValueError("This agent is not allowed to delegate tasks.")

        if sub_agent not in self.sub_agents:
            raise ValueError("The specified sub-agent is not in the list of allowed sub-agents.")

        self.logger.info(f"Delegating task '{task}' to sub-agent: {sub_agent.role}")
        try:
            response = sub_agent.run(task)  # Assuming sub_agent has a run method
            self.logger.info(f"Sub-agent response: {response}")
            return response
        except Exception as e:
            self.logger.error(f"Error delegating task: {e}")
            return "Error: Task delegation failed."

    def run(self, input: str) -> str:
        """Runs the agent."""
        self.logger.info(f"Agent received input: {input}")
        try:
            agent_executor = self.create_agent_executor()
            output = agent_executor.invoke({"input": input})
            self.logger.info(f"Agent raw output: {output}")  # Log raw output

            # Attempt to parse JSON and validate structure
            try:
                json_output = json.loads(output)
                # Determine the expected output class based on self.output_format
                if self.output_format == "SummaryOutput":
                    output_obj = SummaryOutput(**json_output)
                elif self.output_format == "FactExtractionOutput":
                    output_obj = FactExtractionOutput(**json_output)
                else:
                    output_obj = StructuredOutput(**json_output)  # Default
                # If parsing and validation succeed, extract the string output
                output = output_obj.json()  # Return JSON string

            except (json.JSONDecodeError, ValidationError) as e:
                self.logger.warning(f"Output JSON decode/validation error: {e}")
                return f"Error: Invalid output format. {e}"

            # Apply guardrail if defined
            if self.guardrail:
                is_valid, guarded_output = self.apply_guardrail(output)
                if not is_valid:
                    self.logger.warning(f"Guardrail failed: {guarded_output}. Retrying...")
                    # Implement retry logic here if needed
                    return "Error: Guardrail violation."  # Or retry with modified input
                return guarded_output  # Return the guarded output
            return output  # Return the normal output

        except Exception as e:
            self.logger.error(f"Agent execution failed: {e}")
            return f"Error: Agent execution failed. {e}"

    def apply_guardrail(self, output: Any) -> Tuple[bool, Any]:
        """Applies the guardrail function to the output."""
        if isinstance(self.guardrail, str):
            # String guardrail is treated as a description; no actual validation
            self.logger.info(f"Guardrail description: {self.guardrail}")
            return True, output  # Consider all output valid
        elif callable(self.guardrail):
            try:
                is_valid, guarded_output = self.guardrail(output)
                return is_valid, guarded_output
            except Exception as e:
                self.logger.error(f"Guardrail function error: {e}")
                return False, f"Guardrail function error: {e}"
        else:
            self.logger.warning("Invalid guardrail type.  Must be a string or callable.")
            return True, output

    def default_guardrail(self, output: str) -> Tuple[bool, str]:
        """Default guardrail: Checks for harmful or inappropriate content."""
        offensive_terms = ["offensive", "harmful", "inappropriate", "illegal", "unethical"]  # Expand this list
        output_lower = output.lower()
        for term in offensive_terms:
            if term in output_lower:
                return False, f"Output contains potentially harmful content: {term}"
        return True, output

# Example Tool with Structured Output
class MyTool(BaseTool):
    name = "my_tool"
    description = "A tool that returns structured output."

    def _run(self, query: str) -> str:
        """Executes the tool."""
        try:
            # Simulate structured output
            structured_data = {"output": f"Tool processed: {query}", "summary": f"Summary of {query}"}
            # Validate the structure
            output_obj = SummaryOutput(**structured_data) # Validate against SummaryOutput
            return output_obj.json()
        except ValidationError as e:
            return f"Tool Error: Invalid structured output. {e}"

    async def _arun(self, query: str) -> str:
        """Asynchronous version of _run."""
        raise NotImplementedError("This tool does not support asynchronous execution")

# Example Usage (assuming you have a Langchain LLM and tools defined)
# from langchain.llms import OpenAI  # Or any other LLM you want to use
# from langchain.tools import DuckDuckGoSearchRun
#
# llm = OpenAI(temperature=0.0) # Replace with your LLM
# tools = [DuckDuckGoSearchRun(), MyTool()] # Include the new tool
#
# # Create a sub-agent
# sub_agent_config = {
#     "role": "Summarization Expert",
#     "goal": "Summarize text concisely.",
#     "backstory": "An expert at summarizing complex information.",
#     "llm": llm,
#     "tools": [],
#     "verbose": False
# }
# sub_agent = Agent(**sub_agent_config)
#
# agent_config = {
#     "role": "Information Aggregator",
#     "goal": "Gather and present information on a topic, delegating summarization.",
#     "backstory": "An agent skilled at finding and organizing information.",
#     "knowledge": "Access to web search and summarization experts.",
#     "llm": llm,
#     "tools": tools,
#     "verbose": True,
#     "date_format": "%Y-%m-%d %H:%M:%S",
#     "allow_delegation": True,
#     "sub_agents": [sub_agent],
#      "output_format": "SummaryOutput",
#     # "guardrail": content_filter  #  content_filter #"This agent avoids offensive language." # Or a callable
# }
#
# agent = Agent(**agent_config)
#
# # Example of delegating a task
# # task = "Summarize the latest news on artificial intelligence."
# # summary = agent.delegate_task(task, sub_agent)
# # print(f"Summary: {summary}")
#
# response = agent.run("What are the latest advancements in AI?")
# print(response)
